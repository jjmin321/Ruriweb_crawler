# Ruriweb_crawler

## Stack
|           |     Crawler      |
|:---------:|:---------:|
| Developer | ì œì •ë¯¼ | 
| Develop Language | GO |  
| Develop Tool     | Visual Studio Code|

## ğŸ§º ë™ê¸°í™” Synchronization
    - mutex.Lock(), mutex.Unlock() vs sync.WaitGroup
    - mutex ì‚¬ìš© ì‹œ ê³ ë£¨í‹´ì´ í•˜ë‚˜ì”© ì‹¤í–‰ë˜ê²Œ ë” ë™ê¸°í™” ê°€ëŠ¥
    - sync.WaitGroupì„ ì‚¬ìš©í•´ mutexë³´ë‹¤ ì¡°ê¸ˆ ë” ë™ê¸°í™”ì— ì‹ ê²½ì“°ì§€ ì•Šì„ ìˆ˜ ìˆëŠ” ê±° ê°™ìŒ.
```go
//ë™ê¸°í™”ë¥¼ ìœ„í•œ ì‘ì—… ê·¸ë£¹ ì„ ì–¸
var wg sync.WaitGroup
```

## ğŸ“ƒ ì›¹ ë©”ì¸í˜ì´ì§€ì—ì„œ ì›í•˜ëŠ” URLíŒŒì‹± í›„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ ìƒì„± Function will use return important URL from Web mainpage
ë§Œì•½ ì›¹ ë¸Œë¼ìš°ì € HTML ì½”ë“œì— aíƒœê·¸ì™€ ë¶€ëª¨íƒœê·¸ê°€ ëª¨ë‘ ìˆëŠ” íƒœê·¸ë¼ë©´ classê°€ rowì¸ì§€ ë¶„ì„ í›„ ë°˜í™˜
```go
//ì²« ë²ˆì§¸ ë°©ë¬¸(ë©”ì¸í˜ì´ì§€) ëŒ€ìƒìœ¼ë¡œ ì›í•˜ëŠ” urlì„ íŒŒì‹± í›„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
func parseMainNodes(n *html.Node) bool {
	if n.DataAtom == atom.A && n.Parent != nil {
		return scrape.Attr(n.Parent, "class") == "row"
	}
	return false
}
```


## ğŸ‘©â€ğŸ’» ì›¹ ë¸Œë¼ìš°ì € ì½”ë“œ ê°–ê³ ì™€ì„œ ë¶„ì„ Analysis HTML code
```go
    //ë©”ì¸ í˜ì´ì§€ Get ë°©ì‹ ìš”ì²­
	response, err := http.Get(urlRoot) //response(ì‘ë‹µ), request(ìš”ì²­)
	errCheck(err)

	//ìš”ì²­ Body ë‹«ê¸°
	defer response.Body.Close()

	//ì‘ë‹µ ë°ì´í„°(HTML)
	root, err := html.Parse(response.Body) //rootì— ë£¨ë¦¬ì›¹ì‚¬ì´íŠ¸ì˜ ë°”ë””ì½”ë“œë¥¼ ì „ë¶€ ë‹¤ ë„£ìŒ.
	errCheck(err)

	//ëŒ€ìƒ URL ì¶”ì¶œ
	urlList := scrape.FindAll(root, parseMainNodes)
```

## ğŸ§µ urlList ê°ì²´ ë¶„ì„ í›„ íŒŒì‹±í•´ì˜¤ê³ ì í•˜ëŠ” ì‚¬ì´íŠ¸ë“¤ ê³ ë£¨í‹´(ì“°ë ˆë“œ) í†µí•´ ì ‘ì† Connect to desirous site with Goroutine(Thread)
    - ì“°ë ˆë“œ í•˜ë‚˜ ë‹¹ í•˜ë‚˜ì”© ì‘ì—… ëŒ€ê¸°ì—´ ì¶”ê°€(ë™ê¸°í™”, wg.Add(1))
```go
//class = rowì¸ íƒœê·¸ë“¤ ì‹¹ ë‹¤ ê¸ì–´ì™€ì„œ for rangeë¡œ ìˆœíšŒ
	for _, link := range urlList {
		//ëŒ€ìƒ Url 1ì°¨ ì¶œë ¥
		// fmt.Println("Main Link : ", link, idx)
		// fmt.Println("TargetUrl : ", scrape.Attr(link, "href"))
		fileName := strings.Replace(scrape.Attr(link, "href"), "https://bbs.ruliweb.com/family/", "", 1)
		fmt.Println("fileName : ", fileName)

		//ì‘ì—… ëŒ€ê¸°ì—´ì— ì¶”ê°€
		wg.Add(1) //Done ê°œìˆ˜ì™€ ì¼ì¹˜
		//ê³ ë£¨í‹´ ì‹œì‘ -> ì‘ì—… ëŒ€ê¸°ì—´ ê°œìˆ˜ì™€ ê°™ì•„ì•¼ í•¨.
		go scrapContents(scrape.Attr(link, "href"), fileName) //href ê°’ìœ¼ë¡œ ê° ë§í¬ì— ë“¤ì–´ê°€ëŠ” ê±°, í…ìŠ¤íŠ¸ íŒŒì¼ ë§Œë“¤ ìˆ«ìë§Œ ê°–ê³ ì˜¤ëŠ” ê±°
    }
```

## ğŸ“ íŒŒì¼ ìƒì„± Create File
    - ì§€ì •í•œ ê²½ë¡œì— íŒŒì¼ì´ ì—†ë‹¤ë©´ ìƒì„±/ìˆë‹¤ë©´ ìˆ˜ì • 
    - í¼ë¯¸ì…˜ì€ 777ë¡œ ì¤Œ.
    - deferí•¨ìˆ˜ë¡œ íŒŒì¼ ì•ˆ ë‹«ìœ¼ë©´ ëŒ€ì°¸ì‚¬ ë°œìƒ(ì£¼ì˜)
```go
//íŒŒì¼ ìŠ¤í¬ë¦¼ ìƒì„±(ì—´ê¸°) -> íŒŒì¼ëª…, ì˜µì…˜, ê¶Œí•œ
    file, err := os.OpenFile("/Users/jejeongmin/documents/go/src/Web_crawler/scrape/"+fn+".txt", os.O_CREATE|os.O_RDWR, os.FileMode(0777))
//ì—ëŸ¬ì²´í¬
    errCheck(err)
//ë©”ì†Œë“œ ì¢…ë£Œ ì‹œ íŒŒì¼ ë‹«ê¸°
	defer file.Close()
```

## ğŸ—’ï¸ .txtíŒŒì¼ì— íŒŒì‹±ë‚´ìš© ë„£ê¸° Input to txt
    - scrape.FindAll ì‚¬ìš©í•´ì„œ ì›í•˜ëŠ” ë‚´ìš©ë§Œ ê¸ì–´ì˜¤ê¸°
    - w.Flushë¡œ ë²„í¼ ë¹„ì›Œì£¼ë©´ì„œ í•œ ë²ˆì— ë‚´ìš© ë‹¤ ë„£ê¸°
```go
    //ì“°ê¸° ë²„í¼ ì„ ì–¸
	w := bufio.NewWriter(file)

	//parseSubNodes í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ì›í•˜ëŠ” ë…¸ë“œ ìˆœíšŒ(Iterator)í•˜ë©´ì„œ ì¶œë ¥
	for _, g := range scrape.FindAll(root, parseSubNodes) {
		//Url ë° í•´ë‹¹ ë°ì´í„° ì¶œë ¥
		fmt.Println("result : ", scrape.Text(g))
		//íŒŒì‹± ë°ì´í„° -> ë²„í¼ì— ê¸°ë¡
		w.WriteString(scrape.Text(g) + "\r\n")
	}
    w.Flush()
```

<img src="./Screenshot/Scrape .png" width="1000">


